{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xKwIVHsxsx__",
    "outputId": "7f4e2039-17f2-4555-e764-a138f356a0ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-tcn\n",
      "  Downloading https://files.pythonhosted.org/packages/a8/5b/31eed031c196dc192eddf346f053ec6a97aefa4b931164fd8665c92a9d7d/keras_tcn-3.1.1-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.6/dist-packages (from keras-tcn) (1.18.5)\n",
      "Collecting keras==2.3.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
      "\r",
      "\u001b[K     |▉                               | 10kB 28.4MB/s eta 0:00:01\r",
      "\u001b[K     |█▊                              | 20kB 14.2MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 30kB 12.6MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 40kB 12.1MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 51kB 7.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 61kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 71kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 81kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 92kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 102kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 112kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 122kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▎                    | 133kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▏                   | 143kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 153kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▉                  | 163kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▊                 | 174kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 184kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 194kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▍              | 204kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▏             | 215kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 225kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 235kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 245kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 256kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▌         | 266kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 276kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 286kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 296kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 307kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 317kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 327kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 337kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 348kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▍ | 358kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 368kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 378kB 8.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1->keras-tcn) (3.13)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1->keras-tcn) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1->keras-tcn) (1.4.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1->keras-tcn) (2.10.0)\n",
      "Collecting keras-applications>=1.0.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 6.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1->keras-tcn) (1.1.2)\n",
      "Installing collected packages: keras-applications, keras, keras-tcn\n",
      "  Found existing installation: Keras 2.4.3\n",
      "    Uninstalling Keras-2.4.3:\n",
      "      Successfully uninstalled Keras-2.4.3\n",
      "Successfully installed keras-2.3.1 keras-applications-1.0.8 keras-tcn-3.1.1\n"
     ]
    }
   ],
   "source": [
    "pip install keras-tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sPN9q9THtAx5",
    "outputId": "230947e4-16e9-472c-82e0-a6b00c2e36a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Input, Model\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.callbacks import History, ModelCheckpoint, TensorBoard\n",
    "import numpy as np\n",
    "import tcn\n",
    "from tcn import TCN, tcn_full_summary\n",
    "import pickle\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from tcn import compiled_tcn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGERASomu-I8"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phcY5PRevHqi"
   },
   "outputs": [],
   "source": [
    "#with open('/content/drive/My Drive/wavenet-test/features.pickle', 'rb') as handle:\n",
    "#    features = pickle.load(handle)\n",
    "    \n",
    "#with open('/content/drive/My Drive/wavenet-test/labels.pickle', 'rb') as handle:\n",
    "#  labels = pickle.load(handle)\n",
    "\n",
    "#!cp '/content/drive/My Drive/wavenet-test/US8K.tar' ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gBguM79zyEFq",
    "outputId": "ecaa2945-604f-48f1-a6d3-b652d37a93a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 28 18:32:08 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   35C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                 ERR! |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqI7BGbPYWIE"
   },
   "outputs": [],
   "source": [
    "## Utils\n",
    "\n",
    "def create_new_model(depth, filters, kernel_size):\n",
    "    input_layer = keras.layers.Input(shape=(341, 513, 2))\n",
    "    conv1 = keras.layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\", kernel_initializer=\"he_normal\", name=\"conv1\", padding=\"same\")(input_layer)\n",
    "    # Stride of (1, 2) -> stride of 2 in feature dimension, reducing feature dimensionality per timestep\n",
    "    conv2 = keras.layers.Conv2D(filters=32, kernel_size=3, strides=(1,2), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(conv1)\n",
    "    \n",
    "    # Slicing 1D Flattening!\n",
    "    feature_distributor = keras.layers.TimeDistributed(keras.layers.Flatten())(conv2)\n",
    "    \n",
    "    # TCN Unit\n",
    "    tcn1 = tcn.TCN(return_sequences=False, kernel_size=(kernel_size), kernel_initializer=\"he_normal\", nb_filters=filters, \n",
    "                   dilations=[2 ** i for i in range(depth)], nb_stacks=2, use_batch_norm=True,\n",
    "                   dropout_rate=0.5, name=\"tcn1\", use_skip_connections=True, padding=\"same\")(feature_distributor)\n",
    "    tcn1 = keras.layers.BatchNormalization()(tcn1)\n",
    " \n",
    "    last_layer = keras.layers.Dense(20, activation=\"softmax\")(tcn1)\n",
    "    \n",
    "    output_layer = keras.layers.Dense(10, activation=\"softmax\")(last_layer)\n",
    "\n",
    "    model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(optimizer, loss=keras.losses.categorical_crossentropy, metrics=[\"categorical_accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def load_model(depth, filters, kernel_size, save_dir=\"./\"):\n",
    "  model = create_new_model(depth, filters, kernel_size)\n",
    "  saved_model = save_dir + \"saved_wavenet_clasifier.h5\"\n",
    "  # model.load here\n",
    "  if os.path.isfile(saved_model):\n",
    "    model.load_weights(saved_model)\n",
    "  else:\n",
    "    print(\"NO MODEL ON DISK, USING BRAND NEW ONE\")\n",
    "  return model\n",
    "\n",
    "def create_callbacks(save_dir=\"./\"):\n",
    "  saved_model = save_dir + \"saved_wavenet_clasifier.h5\"\n",
    "  saved_hist  = save_dir + 'wavenet_classifier_training_history.csv'\n",
    "  checkpointer = ModelCheckpoint(filepath=saved_model, monitor='val_categorical_accuracy', verbose=1, save_best_only=True)\n",
    "  #tensorboard  = TensorBoard(log_dir=save_dir)\n",
    "  history_cb = tf.keras.callbacks.CSVLogger(saved_hist, separator=\",\", append=True)\n",
    "\n",
    "  return [checkpointer, history_cb] # add tensorboard if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QhEhWKmnZQ7O",
    "outputId": "89110364-f399-490c-e216-e825ca3b9404"
   },
   "outputs": [],
   "source": [
    "all_X = []\n",
    "all_Y = []\n",
    "fulldatasetpath = './'\n",
    "metadata = pd.read_csv('http://76.213.149.126:8081/metadata/UrbanSound8K.csv')\n",
    "print(metadata)\n",
    "for index, row in metadata.iterrows():\n",
    "    \n",
    "    file_name = 'fold'+str(row[\"fold\"])+'/' + str(row[\"slice_file_name\"])\n",
    "    all_X.append(file_name.replace(\".wav\", \"_x2.pkl\"))\n",
    "    all_Y.append(file_name.replace(\".wav\", \"_y2.pkl\"))\n",
    "\n",
    "import requests\n",
    "from multiprocessing.pool import ThreadPool\n",
    " \n",
    "def download_url(url):\n",
    "  #print(\"downloading: \",url)\n",
    "  # assumes that the last segment after the / represents the file name\n",
    "  # if url is abc/xyz/file.txt, the file name will be file.txt\n",
    "  path = url.split(\"/\")\n",
    "  path = path[-2] + \"/\" + path[-1]\n",
    " \n",
    "  r = requests.get(url, stream=True)\n",
    "  if r.status_code == requests.codes.ok:\n",
    "    if not os.path.exists(os.path.dirname(path)):\n",
    "      try:\n",
    "          os.makedirs(os.path.dirname(path))\n",
    "      except OSError as exc: # Guard against race condition\n",
    "          if exc.errno != errno.EEXIST:\n",
    "              raise\n",
    "    with open(path, 'wb') as f:\n",
    "      for data in r:\n",
    "        f.write(data)\n",
    "  return url\n",
    "\n",
    "# Old code for a random split. This inflates accuracy, which is undesirable. \n",
    "# from sklearn.model_selection import train_test_split \n",
    "# x_train, x_test, y_train, y_test = train_test_split(all_X, all_Y, test_size=0.2, random_state = 42)\n",
    "\n",
    "x_train = []\n",
    "x_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "for x in all_X:\n",
    "  if x[0:5] != \"fold5\":\n",
    "    x_train.append(x)\n",
    "  else:\n",
    "    x_test.append(x)\n",
    "\n",
    "\n",
    "for y in all_Y:\n",
    "  if y[0:5] != \"fold5\":\n",
    "    y_train.append(y)\n",
    "  else:\n",
    "    y_test.append(y)\n",
    "    \n",
    "# ^^^ change \"fold5\" to other folds when doing final 10-fold crossvalidation    \n",
    "\n",
    "def file_to_arr(t: tf.Tensor):\n",
    "  x = []\n",
    "  y = []\n",
    "\n",
    "  urls = []\n",
    "\n",
    "  for a in t:\n",
    "    a = np.array(a)\n",
    "    # check whether we have them\n",
    "    # if not, schedule download\n",
    "    if not os.path.exists(a[0]):\n",
    "      urls.append(\"http://76.213.149.126:8081/audio/\" + a[0].decode('UTF-8'))\n",
    "    if not os.path.exists(a[1]):\n",
    "      urls.append(\"http://76.213.149.126:8081/audio/\" + a[1].decode('UTF-8'))\n",
    "  \n",
    "  if (len(urls) > 0):\n",
    "    ThreadPool(8).map(download_url, urls)\n",
    "\n",
    "  for a in t:\n",
    "    a = np.array(a)\n",
    "    #print(a[0])\n",
    "    try:\n",
    "      x.append(pickle.load(open(a[0], 'rb')))\n",
    "      y.append(pickle.load(open(a[1], 'rb')))\n",
    "    except:\n",
    "      os.remove(a[0])\n",
    "      os.remove(a[1])\n",
    "      return file_to_arr(t)\n",
    "\n",
    "  x = np.array(x)\n",
    "  x = np.swapaxes(x, 1, 2)\n",
    "  y = np.array(y).astype(int)\n",
    "  res = (x,y)\n",
    "  #print (res)\n",
    "  return (res)\n",
    "\n",
    "print((list(zip(x_train, y_train))))\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(list(zip(x_train, y_train)))\n",
    "train_ds = train_ds.shuffle(len(train_ds),seed=42)\n",
    "train_ds = train_ds.batch(8)\n",
    "train_ds = train_ds.map((lambda x: tf.py_function(func=file_to_arr,inp=[x], Tout=(tf.float32, tf.int64))))\n",
    "train_ds = train_ds.prefetch(128)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(list(zip(x_test, y_test)))\n",
    "test_ds = test_ds.batch(8)\n",
    "test_ds = test_ds.map((lambda x: tf.py_function(func=file_to_arr,inp=[x], Tout=(tf.float32, tf.int64))))\n",
    "test_ds = test_ds.prefetch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUR4vnrGlV49"
   },
   "outputs": [],
   "source": [
    "#model = create_new_model(7, 20, 6)\n",
    "model = load_model(7, 20, 6, \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qFmf6xmbtHjj",
    "outputId": "80b2af61-38ff-4daf-d2ad-94367f6bb25d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 341, 513, 2)]     0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 341, 513, 32)      608       \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 341, 257, 32)      9248      \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 341, 8224)         0         \n",
      "_________________________________________________________________\n",
      "tcn1 (TCN)                   (None, 20)                1218980   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 1,229,546\n",
      "Trainable params: 1,228,386\n",
      "Non-trainable params: 1,160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#callbacks = create_callbacks(\"./drive/My Drive/wavenet-test/new2/7_20_3_1stack_noskip/\")\n",
    "callbacks = create_callbacks(\"./\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FSx_1_XYh8oU"
   },
   "outputs": [],
   "source": [
    "!mkdir ./drive/My\\ Drive/wavenet-test/new2/7_20_3_1stack_noskip/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zxV56ElGt2rK",
    "outputId": "35501bf0-72cb-41b2-e7a1-4debed6b9135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "975/975 [==============================] - ETA: 0s - loss: 1.2369 - categorical_accuracy: 0.5535\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.51709, saving model to ./saved_wavenet_clasifier.h5\n",
      "975/975 [==============================] - 307s 315ms/step - loss: 1.2369 - categorical_accuracy: 0.5535 - val_loss: 1.4073 - val_categorical_accuracy: 0.5171\n",
      "Epoch 2/300\n",
      "975/975 [==============================] - ETA: 0s - loss: 1.1795 - categorical_accuracy: 0.5713\n",
      "Epoch 00002: val_categorical_accuracy did not improve from 0.51709\n",
      "975/975 [==============================] - 318s 326ms/step - loss: 1.1795 - categorical_accuracy: 0.5713 - val_loss: 1.5024 - val_categorical_accuracy: 0.5128\n",
      "Epoch 3/300\n",
      "975/975 [==============================] - ETA: 0s - loss: 1.1410 - categorical_accuracy: 0.5848\n",
      "Epoch 00003: val_categorical_accuracy did not improve from 0.51709\n",
      "975/975 [==============================] - 315s 323ms/step - loss: 1.1410 - categorical_accuracy: 0.5848 - val_loss: 1.4160 - val_categorical_accuracy: 0.5000\n",
      "Epoch 4/300\n",
      "975/975 [==============================] - ETA: 0s - loss: 1.0901 - categorical_accuracy: 0.6027\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.51709 to 0.59509, saving model to ./saved_wavenet_clasifier.h5\n",
      "975/975 [==============================] - 317s 325ms/step - loss: 1.0901 - categorical_accuracy: 0.6027 - val_loss: 1.2831 - val_categorical_accuracy: 0.5951\n",
      "Epoch 5/300\n",
      "975/975 [==============================] - ETA: 0s - loss: 1.0221 - categorical_accuracy: 0.6306\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.59509\n",
      "975/975 [==============================] - 320s 328ms/step - loss: 1.0221 - categorical_accuracy: 0.6306 - val_loss: 1.3161 - val_categorical_accuracy: 0.5566\n",
      "Epoch 6/300\n",
      "975/975 [==============================] - ETA: 0s - loss: 0.9947 - categorical_accuracy: 0.6396\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.59509\n",
      "975/975 [==============================] - 316s 324ms/step - loss: 0.9947 - categorical_accuracy: 0.6396 - val_loss: 1.3304 - val_categorical_accuracy: 0.5759\n",
      "Epoch 7/300\n",
      "975/975 [==============================] - ETA: 0s - loss: 0.9452 - categorical_accuracy: 0.6677\n",
      "Epoch 00007: val_categorical_accuracy improved from 0.59509 to 0.59615, saving model to ./saved_wavenet_clasifier.h5\n",
      "975/975 [==============================] - 319s 328ms/step - loss: 0.9452 - categorical_accuracy: 0.6677 - val_loss: 1.2709 - val_categorical_accuracy: 0.5962\n",
      "Epoch 8/300\n",
      "975/975 [==============================] - ETA: 0s - loss: 0.9156 - categorical_accuracy: 0.6755\n",
      "Epoch 00008: val_categorical_accuracy improved from 0.59615 to 0.62821, saving model to ./saved_wavenet_clasifier.h5\n",
      "975/975 [==============================] - 318s 326ms/step - loss: 0.9156 - categorical_accuracy: 0.6755 - val_loss: 1.2461 - val_categorical_accuracy: 0.6282\n",
      "Epoch 9/300\n",
      "975/975 [==============================] - ETA: 0s - loss: 0.8485 - categorical_accuracy: 0.7036\n",
      "Epoch 00009: val_categorical_accuracy did not improve from 0.62821\n",
      "975/975 [==============================] - 318s 326ms/step - loss: 0.8485 - categorical_accuracy: 0.7036 - val_loss: 1.3691 - val_categorical_accuracy: 0.5962\n",
      "Epoch 10/300\n",
      "975/975 [==============================] - ETA: 0s - loss: 0.8130 - categorical_accuracy: 0.7125\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.62821\n",
      "975/975 [==============================] - 316s 324ms/step - loss: 0.8130 - categorical_accuracy: 0.7125 - val_loss: 1.2708 - val_categorical_accuracy: 0.5929\n",
      "Epoch 11/300\n",
      "975/975 [==============================] - ETA: 0s - loss: 0.7789 - categorical_accuracy: 0.7314\n",
      "Epoch 00011: val_categorical_accuracy improved from 0.62821 to 0.65919, saving model to ./saved_wavenet_clasifier.h5\n",
      "975/975 [==============================] - 317s 325ms/step - loss: 0.7789 - categorical_accuracy: 0.7314 - val_loss: 1.1855 - val_categorical_accuracy: 0.6592\n",
      "Epoch 12/300\n",
      "975/975 [==============================] - ETA: 0s - loss: 0.7546 - categorical_accuracy: 0.7341\n",
      "Epoch 00012: val_categorical_accuracy did not improve from 0.65919\n",
      "975/975 [==============================] - 319s 327ms/step - loss: 0.7546 - categorical_accuracy: 0.7341 - val_loss: 1.1784 - val_categorical_accuracy: 0.6560\n",
      "Epoch 13/300\n",
      "975/975 [==============================] - ETA: 0s - loss: 0.7302 - categorical_accuracy: 0.7477\n",
      "Epoch 00013: val_categorical_accuracy did not improve from 0.65919\n",
      "975/975 [==============================] - 317s 326ms/step - loss: 0.7302 - categorical_accuracy: 0.7477 - val_loss: 1.2461 - val_categorical_accuracy: 0.6335\n",
      "Epoch 14/300\n",
      "975/975 [==============================] - ETA: 0s - loss: 0.6982 - categorical_accuracy: 0.7630\n",
      "Epoch 00014: val_categorical_accuracy did not improve from 0.65919\n",
      "975/975 [==============================] - 319s 327ms/step - loss: 0.6982 - categorical_accuracy: 0.7630 - val_loss: 1.2630 - val_categorical_accuracy: 0.6442\n",
      "Epoch 15/300\n",
      "975/975 [==============================] - ETA: 0s - loss: 0.6700 - categorical_accuracy: 0.7773\n",
      "Epoch 00015: val_categorical_accuracy did not improve from 0.65919\n",
      "975/975 [==============================] - 319s 327ms/step - loss: 0.6700 - categorical_accuracy: 0.7773 - val_loss: 1.2717 - val_categorical_accuracy: 0.6581\n",
      "Epoch 16/300\n",
      "975/975 [==============================] - ETA: 0s - loss: 0.6481 - categorical_accuracy: 0.7848\n",
      "Epoch 00016: val_categorical_accuracy did not improve from 0.65919\n",
      "975/975 [==============================] - 318s 326ms/step - loss: 0.6481 - categorical_accuracy: 0.7848 - val_loss: 1.3204 - val_categorical_accuracy: 0.6229\n",
      "Epoch 17/300\n",
      "975/975 [==============================] - ETA: 0s - loss: 0.6060 - categorical_accuracy: 0.7989\n",
      "Epoch 00017: val_categorical_accuracy improved from 0.65919 to 0.66453, saving model to ./saved_wavenet_clasifier.h5\n",
      "975/975 [==============================] - 318s 327ms/step - loss: 0.6060 - categorical_accuracy: 0.7989 - val_loss: 1.1977 - val_categorical_accuracy: 0.6645\n",
      "Epoch 18/300\n",
      "975/975 [==============================] - ETA: 0s - loss: 0.5940 - categorical_accuracy: 0.8091\n",
      "Epoch 00018: val_categorical_accuracy did not improve from 0.66453\n",
      "975/975 [==============================] - 319s 327ms/step - loss: 0.5940 - categorical_accuracy: 0.8091 - val_loss: 1.2863 - val_categorical_accuracy: 0.6581\n",
      "Epoch 19/300\n",
      "975/975 [==============================] - ETA: 0s - loss: 0.5508 - categorical_accuracy: 0.8291\n",
      "Epoch 00019: val_categorical_accuracy did not improve from 0.66453\n",
      "975/975 [==============================] - 318s 327ms/step - loss: 0.5508 - categorical_accuracy: 0.8291 - val_loss: 1.2459 - val_categorical_accuracy: 0.6506\n",
      "Epoch 20/300\n",
      "975/975 [==============================] - ETA: 0s - loss: 0.5279 - categorical_accuracy: 0.8371\n",
      "Epoch 00020: val_categorical_accuracy did not improve from 0.66453\n",
      "975/975 [==============================] - 318s 326ms/step - loss: 0.5279 - categorical_accuracy: 0.8371 - val_loss: 1.3250 - val_categorical_accuracy: 0.6303\n",
      "Epoch 21/300\n",
      "975/975 [==============================] - ETA: 0s - loss: 0.5261 - categorical_accuracy: 0.8330\n",
      "Epoch 00021: val_categorical_accuracy did not improve from 0.66453\n",
      "975/975 [==============================] - 317s 325ms/step - loss: 0.5261 - categorical_accuracy: 0.8330 - val_loss: 1.3589 - val_categorical_accuracy: 0.6271\n",
      "Epoch 22/300\n",
      "975/975 [==============================] - ETA: 0s - loss: 0.5078 - categorical_accuracy: 0.8416\n",
      "Epoch 00022: val_categorical_accuracy did not improve from 0.66453\n",
      "975/975 [==============================] - 318s 326ms/step - loss: 0.5078 - categorical_accuracy: 0.8416 - val_loss: 1.2820 - val_categorical_accuracy: 0.6485\n",
      "Epoch 23/300\n",
      "405/975 [===========>..................] - ETA: 2:27 - loss: 0.4602 - categorical_accuracy: 0.8580"
     ]
    }
   ],
   "source": [
    "model.fit(train_ds, validation_data=test_ds, callbacks=callbacks, epochs=300)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Attempt 6 22khz tcn conv.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
